{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1500d6ba-bcaa-4e5f-8ad1-e92eefcfae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from enum import Enum\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe57e864-4dfb-4deb-9926-e7179b72c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = 4\n",
    "COLS = 4\n",
    "type State = Tuple[int, int]\n",
    "type Reward = int\n",
    "\n",
    "Actions = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, rows=ROWS, cols=COLS):\n",
    "        self._rows = rows\n",
    "        self._cols = cols\n",
    "\n",
    "        self._grid = np.array((rows, cols))\n",
    "        self._terminal_states = [(0, 0), (rows-1, cols-1)]\n",
    "\n",
    "    def get_actions(self, state: State) -> List[Tuple[int, State, Reward]]:\n",
    "        if state in self._terminal_states:\n",
    "            return []\n",
    "\n",
    "        output = []\n",
    "        for i, action in enumerate(Actions):\n",
    "            new_r = state[0] + action[0]\n",
    "            new_c = state[1] + action[1]\n",
    "\n",
    "            new_state = (new_r, new_c)\n",
    "            if self._is_valid_state(new_state):\n",
    "                output.append((i, new_state, -1))\n",
    "            else:\n",
    "                output.append((i, state, -1))\n",
    "        return output\n",
    "\n",
    "    def _is_valid_state(self, state) -> bool:\n",
    "        if 0 <= state[0] < self._rows and 0 <= state[1] < self._cols:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb5a092b-31e3-4a8b-af48-e9a20fbef5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 1e-6\n",
    "gridworld = GridWorld()\n",
    "\n",
    "# rows * cols * 4 grid is a policy, each cell represents p(a|s)\n",
    "type Policy = np.Array\n",
    "\n",
    "equirandom = np.zeros((ROWS, COLS, 4)) + 0.25\n",
    "equirandom[0, 0] = 0\n",
    "equirandom[ROWS-1, COLS-1] = 0\n",
    "\n",
    "def policy_evaluation_inplace(policy: Policy, threshold=THRESHOLD):\n",
    "    V = np.zeros((ROWS, COLS))\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        \n",
    "        delta = 0\n",
    "\n",
    "        for r in range(ROWS):\n",
    "            for c in range(COLS):\n",
    "                old = V[r, c]\n",
    "                actions = gridworld.get_actions((r, c))\n",
    "                #print(r, c, actions)\n",
    "                # the double sum from the bellman equation collapses here, since there is only one s',r for each action\n",
    "                V[r,c] = sum([policy[r, c, action]*(reward + V[new_state]) for action, new_state, reward in actions])\n",
    "                delta = max(delta, abs(old-V[r,c]))\n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "        if (iterations %100 == 0):\n",
    "            print(V)\n",
    "        if delta < threshold:\n",
    "            print(iterations)\n",
    "\n",
    "            break\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2abbc2de-485b-4456-964e-d51209d76832",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.         -13.99765839 -19.99663362 -21.99629468]\n",
      " [-13.99765839 -17.99712654 -19.99688008 -19.99691576]\n",
      " [-19.99663362 -19.99688008 -17.99736736 -13.99803444]\n",
      " [-21.99629468 -19.99691576 -13.99803444   0.        ]]\n",
      "167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.        , -13.99999335, -19.99999044, -21.99998948],\n",
       "       [-13.99999335, -17.99999184, -19.99999114, -19.99999125],\n",
       "       [-19.99999044, -19.99999114, -17.99999253, -13.99999442],\n",
       "       [-21.99998948, -19.99999125, -13.99999442,   0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_evaluation_inplace(equirandom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e90e3d5-fb7d-4b64-9a96-491c6779a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(old_policy: Policy, V) -> Tuple[bool, Policy]:\n",
    "    # select a deterministic policy that is greedy wrt V, given a deterministic old_policy\n",
    "    new_policy = np.zeros((ROWS, COLS, 4))\n",
    "    stable_policy = True\n",
    "    for r in range(ROWS):\n",
    "        for c in range(COLS):\n",
    "            old_greedy = np.argmax(old_policy[r, c])\n",
    "            \n",
    "            actions = gridworld.get_actions((r, c))\n",
    "            if not actions:\n",
    "                continue\n",
    "                                           \n",
    "            one_step_value = {i: reward + V[new_state] for i, new_state, reward in actions}\n",
    "\n",
    "            greedy_action = max(one_step_value, key=one_step_value.get)\n",
    "            new_policy[r, c, greedy_action] = 1\n",
    "\n",
    "            # make sure that we actually have something different. \n",
    "            # There can be multiple greedy actions, don't want to get stuck in a loop\n",
    "            if abs(one_step_value[old_greedy] - one_step_value[greedy_action]) >= THRESHOLD:\n",
    "                stable_policy = False\n",
    "\n",
    "            \n",
    "    return stable_policy, new_policy\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4fdcf85-21e6-451f-8b66-bbe512610814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration():\n",
    "    # everything goes up, lol\n",
    "    equirandom = np.zeros((ROWS, COLS, 4)) + 0.25\n",
    "    equirandom[0, 0] = 0\n",
    "    equirandom[ROWS-1, COLS-1] = 0\n",
    "    \n",
    "    V = policy_evaluation_inplace(equirandom)\n",
    "    stable_policy = False\n",
    "    policy = equirandom\n",
    "    \n",
    "    while not stable_policy:\n",
    "        V = policy_evaluation_inplace(policy)\n",
    "        print(V)\n",
    "        stable_policy, policy = policy_improvement(policy, V)\n",
    "        print(\"======\")\n",
    "\n",
    "    print(\"Done Policy Iteration!\")\n",
    "    print(policy)\n",
    "    print(V)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55fa7f52-8352-40dd-aa5f-b2ed55f000c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.         -13.99765839 -19.99663362 -21.99629468]\n",
      " [-13.99765839 -17.99712654 -19.99688008 -19.99691576]\n",
      " [-19.99663362 -19.99688008 -17.99736736 -13.99803444]\n",
      " [-21.99629468 -19.99691576 -13.99803444   0.        ]]\n",
      "167\n",
      "[[  0.         -13.99765839 -19.99663362 -21.99629468]\n",
      " [-13.99765839 -17.99712654 -19.99688008 -19.99691576]\n",
      " [-19.99663362 -19.99688008 -17.99736736 -13.99803444]\n",
      " [-21.99629468 -19.99691576 -13.99803444   0.        ]]\n",
      "167\n",
      "[[  0.         -13.99999335 -19.99999044 -21.99998948]\n",
      " [-13.99999335 -17.99999184 -19.99999114 -19.99999125]\n",
      " [-19.99999044 -19.99999114 -17.99999253 -13.99999442]\n",
      " [-21.99998948 -19.99999125 -13.99999442   0.        ]]\n",
      "======\n",
      "3\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "======\n",
      "Done Policy Iteration!\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1442cf1-a83a-4e9b-99ba-d91ef80213af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(threshold=THRESHOLD):\n",
    "    V = np.zeros((ROWS, COLS))\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        \n",
    "        delta = 0\n",
    "\n",
    "        for r in range(ROWS):\n",
    "            for c in range(COLS):\n",
    "                old = V[r, c]\n",
    "                actions = gridworld.get_actions((r, c))\n",
    "                if not actions:\n",
    "                    continue\n",
    "                # the double sum from the bellman equation collapses here, since there is only one s',r for each action\n",
    "                V[r,c] = max([(reward + V[new_state]) for _, new_state, reward in actions])\n",
    "                delta = max(delta, abs(old-V[r,c]))\n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "        if (iterations % 100 == 0):\n",
    "            print(V)\n",
    "        if delta < threshold:\n",
    "            print(iterations)\n",
    "            break\n",
    "\n",
    "    zero_policy = np.zeros((ROWS, COLS, 4))\n",
    "    _, policy = policy_improvement(zero_policy, V)\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b042f57b-b172-4b7d-894f-60818bf7e7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0., -1., -2., -3.],\n",
       "        [-1., -2., -3., -2.],\n",
       "        [-2., -3., -2., -1.],\n",
       "        [-3., -2., -1.,  0.]]),\n",
       " array([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 1., 0.]],\n",
       " \n",
       "        [[1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.]],\n",
       " \n",
       "        [[1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.]],\n",
       " \n",
       "        [[1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab6830c-553b-45b3-9465-2b61f396b957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
